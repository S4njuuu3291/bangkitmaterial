{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memilih Fungsi Aktivasi untuk Neuron dalam Jaringan Saraf\n",
    "\n",
    "Mari kita lihat bagaimana Anda dapat memilih **fungsi aktivasi** untuk neuron yang berbeda di jaringan saraf Anda. Kita akan mulai dengan beberapa panduan tentang cara memilihnya untuk lapisan keluaran. Ternyata, tergantung pada label target atau label kebenaran dasar $ y $, akan ada satu pilihan yang cukup alami untuk fungsi aktivasi di lapisan keluaran. Kemudian, kita akan memperhatikan pilihan fungsi aktivasi juga untuk lapisan tersembunyi jaringan saraf Anda.\n",
    "\n",
    "<img src=\"img\\image-6.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "### Fungsi Aktivasi untuk Lapisan Keluaran\n",
    "\n",
    "Anda dapat memilih aktivasi yang berbeda untuk neuron yang berbeda di jaringan saraf Anda. Ketika mempertimbangkan fungsi aktivasi untuk lapisan keluaran, biasanya ada satu pilihan yang cukup alami, tergantung pada apa yang menjadi target atau label kebenaran dasar $ y $. \n",
    "\n",
    "#### 1. Masalah Klasifikasi Biner\n",
    "\n",
    "Jika Anda sedang mengerjakan **masalah klasifikasi biner** di mana $ y $ adalah 0 atau 1, maka fungsi aktivasi **sigmoid** hampir selalu akan menjadi pilihan yang paling alami. Ini karena jaringan saraf belajar untuk memprediksi probabilitas bahwa $ y $ sama dengan satu, seperti yang kita miliki untuk regresi logistik. \n",
    "\n",
    "**Rekomendasi**: Jika Anda bekerja pada masalah klasifikasi biner, gunakan **sigmoid** pada lapisan keluaran.\n",
    "\n",
    "#### 2. Masalah Regresi\n",
    "\n",
    "Jika Anda mencoba memprediksi bagaimana **harga saham** besok akan berubah dibandingkan dengan harga saham hari ini, maka $ y $ bisa menjadi angka yang positif atau negatif. Dalam kasus ini, saya merekomendasikan Anda menggunakan **fungsi aktivasi linier**. \n",
    "\n",
    "Mengapa demikian? Karena output jaringan saraf $ f(x) $ akan menjadi $ g $ yang diterapkan pada $ z $, dan dengan fungsi aktivasi linier, $ g(z) $ dapat mengambil salah satu nilai positif atau negatif.\n",
    "\n",
    "#### 3. Nilai Non-Negatif\n",
    "\n",
    "Jika $ y $ hanya dapat mengambil nilai non-negatif, seperti jika Anda memprediksi **harga rumah** yang tidak akan pernah negatif, maka hal yang paling wajar adalah memilih **fungsi aktivasi ReLU**. Fungsi aktivasi ini hanya mengambil nilai non-negatif, baik nol atau nilai positif.\n",
    "\n",
    "### Memilih Fungsi Aktivasi untuk Lapisan Tersembunyi\n",
    "\n",
    "Untuk lapisan tersembunyi jaringan saraf, **fungsi aktivasi ReLU** merupakan pilihan yang paling umum digunakan. Meskipun sebelumnya kami menggambarkan jaringan saraf menggunakan fungsi aktivasi sigmoid, perkembangan di bidang ini telah menunjukkan bahwa ReLU lebih sering digunakan, sedangkan sigmoid hampir tidak pernah.\n",
    "\n",
    "**Pengecualian**: Anda menggunakan fungsi aktivasi sigmoid di lapisan keluaran jika Anda memiliki masalah klasifikasi biner.\n",
    "\n",
    "### Mengapa Menggunakan ReLU?\n",
    "\n",
    "1. **Kecepatan Perhitungan**: ReLU sedikit lebih cepat untuk dihitung karena hanya membutuhkan perhitungan $ \\max(0, z) $, sedangkan sigmoid memerlukan perhitungan eksponensial.\n",
    "   \n",
    "2. **Gradien Datar**: ReLU hanya menjadi datar dalam satu bagian dari grafik. Sebaliknya, fungsi sigmoid menjadi datar di dua tempat, sehingga ketika Anda menggunakan penurunan gradien untuk melatih jaringan saraf, ini dapat memperlambat pembelajaran.\n",
    "\n",
    "### Rekomendasi Akhir\n",
    "\n",
    "<img src=\"img\\image-7.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "**Singkatnya**, berikut adalah rekomendasi saya dalam memilih fungsi aktivasi untuk jaringan saraf Anda:\n",
    "- **Lapisan Keluaran**:\n",
    "  - **Sigmoid** untuk masalah klasifikasi biner.\n",
    "  - **Linier** jika $ y $ adalah angka yang bisa positif atau negatif.\n",
    "  - **ReLU** jika $ y $ hanya dapat mengambil nilai positif atau nol.\n",
    "  \n",
    "- **Lapisan Tersembunyi**:\n",
    "  - Gunakan **ReLU** sebagai fungsi aktivasi default.\n",
    "\n",
    "### Sintaks di TensorFlow\n",
    "\n",
    "Di TensorFlow, ini adalah cara Anda akan menerapkannya. Untuk lapisan tersembunyi, gunakan fungsi aktivasi ReLU, dan untuk lapisan keluaran, Anda dapat menggunakan fungsi aktivasi sigmoid atau linier. \n",
    "\n",
    "Dengan rangkaian fungsi aktivasi yang lebih kaya ini, Anda akan diposisikan dengan baik untuk membangun jaringan saraf yang jauh lebih kuat daripada hanya menggunakan fungsi aktivasi sigmoid.\n",
    "\n",
    "### Fungsi Aktivasi Lainnya\n",
    "\n",
    "Jika Anda melihat literatur penelitian, Anda terkadang mendengar tentang fungsi aktivasi lain seperti:\n",
    "- **TanH**\n",
    "- **Leaky ReLU**\n",
    "- **Swish**\n",
    "\n",
    "Setiap beberapa tahun, peneliti menemukan fungsi aktivasi baru yang terkadang bekerja sedikit lebih baik. Namun, dalam kebanyakan kasus yang Anda pelajari, fungsi-fungsi ini sudah cukup bagus. \n",
    "\n",
    "### Pertanyaan Selanjutnya\n",
    "\n",
    "Namun, hal ini masih menimbulkan pertanyaan lain: Mengapa kita perlu fungsi aktivasi sama sekali? Mengapa tidak menggunakan fungsi aktivasi linier saja? Di video berikutnya, mari kita lihat mengapa fungsi aktivasi sangat penting untuk mendapatkan jaringan saraf Anda bekerja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
