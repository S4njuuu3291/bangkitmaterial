{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Aktivasi dalam Jaringan Saraf\n",
    "\n",
    "Sejauh ini, kami telah menggunakan **fungsi aktivasi sigmoid** di semua node dalam lapisan tersembunyi dan lapisan keluaran. Kami memulainya dengan cara itu karena kami membangun jaringan saraf dengan mengambil regresi logistik dan membuat banyak unit regresi logistik serta merangkainya bersama-sama. Namun, jika Anda menggunakan fungsi aktivasi lain, jaringan saraf Anda dapat menjadi jauh lebih kuat. Mari kita lihat cara melakukannya.\n",
    "\n",
    "<img src=\"img\\image-5.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "### Contoh Prediksi Permintaan\n",
    "\n",
    "Ingat kembali contoh prediksi permintaan dari minggu lalu, di mana dengan harga, biaya pengiriman, pemasaran, dan material, Anda akan mencoba memprediksi apakah sesuatu sangat terjangkau. Jika ada **kesadaran yang baik** dan **kualitas yang dirasakan tinggi**, cobalah untuk memprediksi apakah itu adalah produk terlaris. Namun, ini mengasumsikan bahwa kesadaran mungkin biner, yaitu orang-orang sadar atau tidak.\n",
    "\n",
    "### Kesadaran yang Tidak Biner\n",
    "\n",
    "Tampaknya tingkat kemungkinan pembeli menyadari kaos yang Anda jual mungkin tidak biner; mereka bisa:\n",
    "- Sedikit sadar\n",
    "- Agak sadar\n",
    "- Sangat sadar\n",
    "- Benar-benar viral\n",
    "\n",
    "Jadi, daripada memodelkan kesadaran sebagai angka biner (0, 1), Anda mencoba memperkirakan **probabilitas kesadaran**. Kesadaran mungkin harus berupa angka non-negatif, karena bisa ada nilai kesadaran yang berkisar dari 0 hingga angka yang sangat besar.\n",
    "\n",
    "### Mengubah Fungsi Aktivasi\n",
    "\n",
    "Sebelumnya, kita telah menggunakan persamaan ini untuk menghitung aktivasi unit tersembunyi kedua yang memperkirakan kesadaran, di mana $ g $ adalah fungsi sigmoid yang hanya berkisar antara 0 dan 1. Jika Anda ingin memungkinkan $ a $ untuk berpotensi mengambil nilai positif yang jauh lebih besar, kita dapat menukar fungsi aktivasi yang berbeda.\n",
    "\n",
    "### Fungsi Aktivasi ReLU\n",
    "\n",
    "Pilihan fungsi aktivasi yang sangat umum dalam jaringan saraf adalah **ReLU (Rectified Linear Unit)**. Fungsi ini terlihat seperti ini:\n",
    "\n",
    "- Jika $ z $ adalah nilai input, maka:\n",
    "  $\n",
    "  g(z) = \\max(0, z)\n",
    "  $\n",
    "\n",
    "Silakan verifikasi sendiri bahwa $ \\max(0, z) $ menghasilkan kurva yang telah digambar di sini. Jika $ 1, 2 $ adalah $ g(z) $ untuk nilai $ z $ ini, maka $ a $, nilai deaktivasi, tidak dapat mengambil nilai 0 atau nilai non-negatif lainnya.\n",
    "\n",
    "Fungsi aktivasi ini memiliki nama, yaitu **ReLU**, yang merupakan singkatan dari **Rectified Linear Unit**. Jangan terlalu khawatir tentang apa yang dimaksud dengan \"diperbaiki\" dan apa yang dimaksud dengan \"unit linier.\" Ini hanyalah nama yang diberikan penulis untuk fungsi aktivasi khusus ini ketika mereka membuatnya.\n",
    "\n",
    "### Pilihan Fungsi Aktivasi\n",
    "\n",
    "Secara umum, Anda memiliki pilihan apa yang akan digunakan untuk $ g(z) $. Terkadang kita akan menggunakan pilihan yang berbeda daripada fungsi aktivasi sigmoid. Berikut adalah fungsi aktivasi yang paling umum digunakan:\n",
    "\n",
    "- **Fungsi Aktivasi Sigmoid**: \n",
    "  $\n",
    "  g(z) = \\text{sigmoid}(z)\n",
    "  $\n",
    "\n",
    "- **Fungsi Aktivasi ReLU**: \n",
    "  $\n",
    "  g(z) = \\max(0, z)\n",
    "  $\n",
    "\n",
    "- **Fungsi Aktivasi Linier**: \n",
    "  $\n",
    "  g(z) = z\n",
    "  $\n",
    "\n",
    "Terkadang, jika Anda menggunakan fungsi aktivasi linier, orang akan mengatakan bahwa kita tidak menggunakan fungsi aktivasi apa pun karena jika $ a $ adalah $ g(z) $ di mana $ g(z) = z $, maka $ a = w \\cdot x + b $. Jadi, seolah-olah tidak ada $ g $ sama sekali di sana.\n",
    "\n",
    "### Kesimpulan\n",
    "\n",
    "Ketiga fungsi ini mungkin merupakan fungsi aktivasi yang paling umum digunakan dalam jaringan saraf. Akhir minggu ini, kita akan membahas fungsi keempat yang disebut **fungsi aktivasi softmax**. Dengan fungsi-fungsi aktivasi ini, Anda akan dapat membangun berbagai macam jaringan saraf yang kuat. \n",
    "\n",
    "Saat membangun jaringan saraf untuk setiap neuron, apakah Anda ingin menggunakan fungsi aktivasi sigmoid, ReLU, atau fungsi aktivasi linier? Bagaimana Anda memilih di antara berbagai fungsi aktivasi ini? Mari kita bahas di video berikutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
