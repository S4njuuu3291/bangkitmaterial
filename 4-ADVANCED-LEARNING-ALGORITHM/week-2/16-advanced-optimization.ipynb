{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descent Gradient**\n",
    "\n",
    "Gradient descent adalah algoritma optimisasi yang banyak digunakan dalam machine learning, dan menjadi dasar dari banyak algoritma seperti regresi linear, regresi logistik, dan implementasi awal jaringan saraf. Namun, sekarang telah ada beberapa algoritma optimisasi lain untuk meminimalkan fungsi biaya yang bahkan lebih baik daripada gradient descent. Dalam video ini, kita akan melihat sebuah algoritma yang dapat membantu melatih jaringan saraf Anda jauh lebih cepat daripada gradient descent. Ingat bahwa ini adalah ekspresi untuk satu langkah gradient descent. Parameter $ w_j $ diperbarui sebagai $ w_j $ dikurangi laju pembelajaran $ \\alpha $ dikali turunan parsial ini. Bagaimana kita bisa membuat ini bekerja lebih baik?\n",
    "\n",
    "<img src=\"img\\image-13.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "**Contoh Gradient Descent**\n",
    "\n",
    "Dalam contoh ini, saya telah memplot fungsi biaya $ J $ menggunakan plot kontur yang terdiri dari elips-elips, dan minimum dari fungsi biaya ini ada di pusat elips di sini. Sekarang, jika Anda memulai gradient descent di sini, satu langkah gradient descent, jika $ \\alpha $ kecil, mungkin akan membawa Anda sedikit ke arah sana. Lalu langkah lain, kemudian langkah lain, lalu langkah lain, dan Anda akan melihat bahwa setiap langkah gradient descent bergerak ke arah yang sama, dan jika Anda melihat ini terjadi, Anda mungkin bertanya-tanya, \"mengapa kita tidak membuat $ \\alpha $ lebih besar, dapatkah kita memiliki algoritma yang secara otomatis meningkatkan $ \\alpha $?\"\n",
    "\n",
    "**Algoritma Adam**\n",
    "\n",
    "Ada algoritma bernama Adam yang dapat melakukan itu. Jika algoritma ini melihat bahwa laju pembelajaran terlalu kecil, dan kita hanya mengambil langkah-langkah kecil ke arah yang sama berulang kali, kita harus membuat laju pembelajaran $ \\alpha $ lebih besar. Sebaliknya, jika laju pembelajaran $ \\alpha $ relatif besar, langkah pertama dari gradient descent mungkin membawa kita ke sini, langkah kedua ke sini, dan seterusnya, dan jika Anda melihat gradient descent melakukan ini, berosilasi bolak-balik, Anda mungkin tergoda untuk berkata, \"mengapa kita tidak membuat laju pembelajaran lebih kecil?\"\n",
    "\n",
    "Adam juga bisa melakukannya secara otomatis, dan dengan laju pembelajaran yang lebih kecil, Anda dapat mengambil jalur yang lebih halus menuju minimum dari fungsi biaya. Tergantung pada bagaimana gradient descent berlangsung, terkadang Anda menginginkan laju pembelajaran yang lebih besar, dan terkadang Anda menginginkan laju pembelajaran yang lebih kecil. Algoritma Adam dapat menyesuaikan laju pembelajaran secara otomatis.\n",
    "\n",
    "<img src=\"img\\image-14.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "**Penjelasan Adam**\n",
    "\n",
    "Adam singkatan dari **Adaptive Moment Estimation**, atau A-D-A-M, dan jangan khawatir terlalu banyak tentang apa arti nama ini, karena ini hanya nama yang diberikan penulis untuk algoritma ini. Namun, menariknya, algoritma Adam tidak menggunakan satu laju pembelajaran global $ \\alpha $. Ia menggunakan laju pembelajaran yang berbeda untuk setiap parameter model Anda. Jika Anda memiliki parameter $ w_1 $ hingga $ w_{10} $, serta $ b $, maka Adam memiliki 11 parameter laju pembelajaran: $ \\alpha_1 $, $ \\alpha_2 $, hingga $ \\alpha_{10} $ untuk $ w_1 $ hingga $ w_{10} $, dan juga $ \\alpha_{11} $ untuk parameter $ b $.\n",
    "\n",
    "Intuisi di balik algoritma Adam adalah, jika parameter $ w_j $ atau $ b $ terus bergerak ke arah yang sama, mari tingkatkan laju pembelajaran untuk parameter tersebut. Sebaliknya, jika parameter terus berosilasi atau bergerak bolak-balik, mari kurangi $ \\alpha_j $ untuk parameter tersebut sedikit.\n",
    "\n",
    "<img src=\"img\\image-15.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "**Implementasi Adam dalam Kode**\n",
    "\n",
    "Rincian bagaimana Adam melakukan ini agak rumit dan berada di luar lingkup kursus ini, tetapi dalam kode, ini adalah cara Anda mengimplementasikannya. Modelnya persis sama seperti sebelumnya, dan cara Anda mengkompilasi model sangat mirip dengan apa yang telah kita pelajari sebelumnya, kecuali bahwa kita sekarang menambahkan satu argumen tambahan ke fungsi kompilasi, yaitu kita menentukan bahwa optimisasi yang Anda inginkan adalah tf.keras.optimizers.Adam optimizer.\n",
    "\n",
    "Adam membutuhkan laju pembelajaran awal default $ \\alpha $, dan dalam contoh ini, saya menetapkan laju pembelajaran awal tersebut sebesar $ 10^{-3} $. Tetapi ketika Anda menggunakan algoritma Adam dalam praktiknya, ada baiknya mencoba beberapa nilai untuk laju pembelajaran global ini. Cobalah beberapa nilai besar dan beberapa nilai yang lebih kecil untuk melihat mana yang memberikan kinerja pembelajaran yang tercepat. Dibandingkan dengan algoritma gradient descent asli yang Anda pelajari sebelumnya, algoritma Adam, karena dapat menyesuaikan laju pembelajaran secara otomatis, lebih tangguh terhadap pemilihan laju pembelajaran yang tepat, meskipun masih ada cara untuk menyesuaikan parameter ini sedikit untuk mendapatkan pembelajaran yang lebih cepat.\n",
    "\n",
    "<img src=\"img\\image-16.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "**Kesimpulan**\n",
    "\n",
    "Itulah algoritma optimisasi Adam. Biasanya bekerja jauh lebih cepat daripada gradient descent, dan telah menjadi standar de facto dalam bagaimana praktisi melatih jaringan saraf mereka. Jika Anda mencoba memutuskan algoritma pembelajaran apa yang akan digunakan untuk melatih jaringan saraf Anda, pilihan yang aman adalah menggunakan algoritma Adam, dan sebagian besar praktisi saat ini akan menggunakan Adam daripada algoritma gradient descent standar. Dengan ini, semoga algoritma pembelajaran Anda dapat belajar jauh lebih cepat.\n",
    "\n",
    "Sekarang, dalam beberapa video berikutnya, saya ingin menyentuh beberapa konsep yang lebih maju untuk jaringan saraf, dan khususnya, dalam video berikutnya, mari kita lihat beberapa tipe lapisan alternatif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
