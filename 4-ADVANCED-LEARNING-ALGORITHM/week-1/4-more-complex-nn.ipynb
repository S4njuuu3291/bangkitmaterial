{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengembangan Jaringan Saraf yang Lebih Kompleks\n",
    "\n",
    "Pada video sebelumnya, Anda belajar tentang lapisan jaringan saraf dan bagaimana lapisan tersebut menerima input berupa vektor angka dan mengeluarkan vektor angka lainnya. Di video ini, kita akan menggunakan lapisan tersebut untuk membangun jaringan saraf yang lebih kompleks. Melalui ini, saya berharap notasi yang kita gunakan untuk jaringan saraf menjadi lebih jelas dan konkret.\n",
    "\n",
    "## Contoh Jaringan Saraf\n",
    "\n",
    "Ini adalah contoh yang akan saya gunakan sepanjang video ini sebagai contoh dari jaringan saraf yang lebih kompleks. Jaringan ini memiliki **empat lapisan**, tidak menghitung lapisan input, yang juga disebut **Lapisan 0**. Lapisan 1, 2, dan 3 adalah lapisan tersembunyi, dan Lapisan 4 adalah lapisan output. Secara konvensional, ketika kita mengatakan bahwa jaringan saraf memiliki empat lapisan, itu termasuk semua lapisan tersembunyi dan lapisan output, tetapi kita tidak menghitung lapisan input.\n",
    "\n",
    "### Fokus pada Lapisan 3\n",
    "\n",
    "Mari kita fokus pada **Lapisan 3**, yang merupakan lapisan tersembunyi ketiga dan terakhir. \n",
    "\n",
    "<img src=\"img\\image-7.png\" alt=\"Contoh Deteksi Mobil\" style=\"display: block; margin-left: auto; margin-right: auto; width: 800px;\">\n",
    "\n",
    "- **Input**: Lapisan 3 menerima input berupa vektor $ a^{(2)} $ yang dihitung oleh lapisan sebelumnya.\n",
    "- **Output**: Output dari Lapisan 3 adalah $ a^{(3)} $, yang merupakan vektor lainnya.\n",
    "\n",
    "## Perhitungan di Lapisan 3\n",
    "\n",
    "Jika terdapat tiga neuron (atau unit tersembunyi), maka ada parameter:\n",
    "- $ w_1, b_1 $\n",
    "- $ w_2, b_2 $\n",
    "- $ w_3, b_3 $\n",
    "\n",
    "Lapisan ini menghitung:\n",
    "- $ a_1 = \\text{sigmoid}(w_1 \\cdot a^{(2)} + b_1) $\n",
    "- $ a_2 = \\text{sigmoid}(w_2 \\cdot a^{(2)} + b_2) $\n",
    "- $ a_3 = \\text{sigmoid}(w_3 \\cdot a^{(2)} + b_3) $\n",
    "\n",
    "Maka, output dari lapisan ini adalah vektor yang terdiri dari $ a_1, a_2, $ dan $ a_3 $. \n",
    "\n",
    "Secara konvensional, jika kita ingin lebih eksplisit menunjukkan bahwa semua ini terkait dengan **Lapisan 3**, kita menambahkan superscript dan tanda kurung siku untuk menunjukkan bahwa parameter $ w $ dan $ b $ terkait dengan neuron di Lapisan 3 dan bahwa aktivasi ini adalah aktivasi dari Lapisan 3.\n",
    "\n",
    "## Memeriksa Pemahaman\n",
    "\n",
    "Untuk memeriksa pemahaman kita, saya akan menyembunyikan superscript dan subscript yang terkait dengan neuron kedua. Tanpa memutar ulang video ini, coba pikirkan tentang apa yang hilang dalam persamaan ini dan isi sendiri. Setelah itu, periksa kuis di akhir video dan lihat apakah Anda dapat menemukan superscript dan subscript yang sesuai.\n",
    "\n",
    "Jika Anda memilih opsi pertama, maka Anda benar! Aktivasi neuron kedua di Lapisan 3 dilambangkan dengan $ a_{3,2} $. Untuk menerapkan fungsi aktivasi $ g $, kita menggunakan parameter dari neuron yang sama. Maka, $ w $ dan $ b $ akan memiliki subscript 2 dan superscript 3.\n",
    "\n",
    "### Fungsi Aktivasi\n",
    "\n",
    "Input fitur akan menjadi vektor output dari lapisan sebelumnya, yaitu Lapisan 2. Jadi itu akan menjadi vektor $ a^{(2)} $. Opsi kedua adalah menggunakan vektor $ a^{(3)} $, yang bukan merupakan vektor output dari lapisan sebelumnya. Input ke lapisan ini adalah $ a^{(2)} $. Opsi ketiga memiliki $ a_{2,2} $ sebagai input, yang merupakan angka tunggal, bukan vektor. \n",
    "\n",
    "Untuk merangkum, $ a_3 $ adalah aktivasi yang terkait dengan Lapisan 3 untuk neuron kedua, sehingga $ a_2 $ adalah parameter yang terkait dengan Lapisan ketiga.\n",
    "\n",
    "## Notasi Umum\n",
    "\n",
    "Notasi umum untuk aktivasi dari unit $ j $ di lapisan $ l $ adalah:\n",
    "$$ a^{(l)}_j = \\text{sigmoid}(w^{(l)} \\cdot a^{(l-1)} + b^{(l)}) $$\n",
    "\n",
    "Di mana:\n",
    "- $ w^{(l)} $ adalah parameter bobot untuk lapisan $ l $\n",
    "- $ a^{(l-1)} $ adalah output dari lapisan sebelumnya\n",
    "- $ b^{(l)} $ adalah parameter bias untuk lapisan $ l $\n",
    "\n",
    "Ketika membangun jaringan saraf, unit $ j $ merujuk pada neuron ke-$ j $. Dalam konteks jaringan saraf, fungsi $ g $ adalah fungsi sigmoid, yang juga dikenal sebagai fungsi aktivasi, karena $ g $ menghasilkan nilai aktivasi.\n",
    "\n",
    "\n",
    "### Penutup\n",
    "\n",
    "Saat ini, satu-satunya fungsi aktivasi yang telah Anda lihat adalah fungsi sigmoid, tetapi minggu depan, kita akan melihat fungsi lainnya. Fungsi aktivasi adalah fungsi yang menghasilkan nilai aktivasi.\n",
    "\n",
    "Sebagai tambahan notasi, saya juga akan memberi nama vektor input $ X $ sebagai $ a_0 $. Dengan cara ini, persamaan yang sama juga berlaku untuk lapisan pertama, di mana ketika $ l = 1 $, aktivasi lapisan pertama adalah:\n",
    "$$ a_1 = \\text{sigmoid}(w^{(1)} \\cdot a_0 + b^{(1)}) $$\n",
    "\n",
    "Dengan notasi ini, Anda sekarang tahu cara menghitung nilai aktivasi dari setiap lapisan dalam jaringan saraf sebagai fungsi dari parameter serta aktivasi dari lapisan sebelumnya.\n",
    "\n",
    "## Algoritma Inferensi\n",
    "\n",
    "Sekarang, kita akan menerapkan ini ke dalam algoritma inferensi untuk jaringan saraf, yaitu cara untuk membuat prediksi. Mari kita lihat itu di video selanjutnya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
