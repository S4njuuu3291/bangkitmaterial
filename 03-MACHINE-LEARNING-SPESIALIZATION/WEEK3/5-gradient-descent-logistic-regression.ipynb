{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent untuk Logistic Regression\n",
    "\n",
    "Dalam **logistic regression**, kita menggunakan **gradient descent** untuk menemukan parameter-model yang optimal (bobot dan bias) dengan cara meminimalkan fungsi biaya. Berikut adalah penjelasan tentang bagaimana **gradient descent** bekerja pada **logistic regression**.\n",
    "\n",
    "### 1. Fungsi Prediksi (Hypothesis) pada Logistic Regression\n",
    "\n",
    "Pada **logistic regression**, kita menggunakan fungsi **sigmoid** sebagai fungsi aktivasi untuk memetakan nilai input menjadi probabilitas antara 0 dan 1. Fungsi ini mengubah hasil dari persamaan linear menjadi probabilitas.\n",
    "\n",
    "Fungsi **sigmoid** ($ \\sigma $) adalah sebagai berikut:\n",
    "$$\n",
    "h_{\\theta}(x) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "di mana $ z $ adalah hasil dari persamaan linear:\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "$$\n",
    "atau lebih ringkasnya:\n",
    "$$\n",
    "z = \\theta^T x\n",
    "$$\n",
    "Di sini:\n",
    "- $ \\theta $ adalah parameter (bobot dan bias) model.\n",
    "- $ x $ adalah fitur atau input.\n",
    "\n",
    "### 2. Fungsi Biaya (Cost Function) untuk Logistic Regression\n",
    "\n",
    "Untuk mengukur seberapa baik model kita memprediksi probabilitas (yaitu seberapa akurat prediksi terhadap label yang sebenarnya), kita menggunakan **binary cross-entropy loss** atau **log loss** sebagai fungsi biaya.\n",
    "\n",
    "Fungsi biaya untuk **logistic regression** adalah sebagai berikut:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ m $ adalah jumlah total data.\n",
    "- $ y^{(i)} $ adalah label aktual (0 atau 1) untuk data ke-i.\n",
    "- $ h_{\\theta}(x^{(i)}) $ adalah prediksi probabilitas untuk data ke-i.\n",
    "\n",
    "Tujuan kita adalah meminimalkan fungsi biaya ini dengan menyesuaikan parameter $ \\theta $.\n",
    "\n",
    "### 3. Gradient Descent untuk Logistic Regression\n",
    "\n",
    "Gradient descent adalah metode yang digunakan untuk meminimalkan fungsi biaya dengan cara mengupdate parameter model ($ \\theta $) secara iteratif.\n",
    "\n",
    "Langkah-langkah untuk menerapkan gradient descent pada logistic regression adalah sebagai berikut:\n",
    "\n",
    "#### 3.1. Perhitungan Gradien (Turunan dari Fungsi Biaya)\n",
    "Untuk mengupdate parameter $ \\theta $, kita perlu menghitung turunan dari fungsi biaya $ J(\\theta) $ terhadap setiap parameter $ \\theta_j $. Gradien atau turunan ini memberi tahu kita arah perubahan yang perlu dilakukan pada parameter untuk mengurangi nilai fungsi biaya.\n",
    "\n",
    "Gradien dari fungsi biaya terhadap $ \\theta_j $ adalah:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ h_{\\theta}(x^{(i)}) $ adalah prediksi model pada data ke-i.\n",
    "- $ y^{(i)} $ adalah label yang sebenarnya.\n",
    "- $ x_j^{(i)} $ adalah fitur ke-j untuk data ke-i.\n",
    "\n",
    "#### 3.2. Update Parameter dengan Gradient Descent\n",
    "\n",
    "Dengan gradien yang telah dihitung, kita dapat mengupdate parameter $ \\theta_j $ dengan rumus:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ \\alpha $ adalah **learning rate**, yaitu seberapa besar langkah perubahan yang kita ambil pada setiap iterasi.\n",
    "- $ \\theta_j $ adalah parameter model yang sedang diupdate.\n",
    "\n",
    "#### 3.3. Proses Iteratif\n",
    "\n",
    "Proses ini akan diulang hingga kita mencapai konvergensi, yaitu ketika perubahan dalam fungsi biaya sangat kecil atau tidak signifikan lagi. Di setiap iterasi, parameter $ \\theta $ diperbarui untuk meminimalkan fungsi biaya.\n",
    "\n",
    "### 4. Contoh Pseudocode untuk Gradient Descent pada Logistic Regression\n",
    "\n",
    "Berikut adalah langkah-langkah dasar pseudocode untuk menerapkan gradient descent pada logistic regression:\n",
    "\n",
    "```python\n",
    "# Pseudocode untuk Gradient Descent Logistic Regression\n",
    "\n",
    "# Inisialisasi parameter\n",
    "theta = np.zeros(n+1)  # n adalah jumlah fitur, ditambah 1 untuk bias\n",
    "\n",
    "# Set learning rate dan jumlah iterasi\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Fungsi untuk menghitung prediksi menggunakan sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Fungsi biaya (cost function)\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    return (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "# Fungsi untuk menghitung gradien\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(np.dot(X, theta))  # Prediksi\n",
    "        gradient = np.dot(X.T, (h - y)) / m  # Gradien\n",
    "        theta -= alpha * gradient  # Update parameter\n",
    "    return theta\n",
    "\n",
    "# Jalankan gradient descent\n",
    "theta = gradient_descent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# Output parameter optimal theta\n",
    "print(\"Parameter optimal:\", theta)\n",
    "```\n",
    "\n",
    "### 5. Kesimpulan\n",
    "\n",
    "- **Logistic Regression** adalah model yang digunakan untuk klasifikasi biner (dua kelas), yang mengoutputkan probabilitas yang dipetakan antara 0 dan 1 dengan menggunakan fungsi sigmoid.\n",
    "- **Gradient Descent** digunakan untuk menyesuaikan parameter model logistic regression ($ \\theta $) dengan cara meminimalkan fungsi biaya (loss function).\n",
    "- Proses gradient descent melibatkan perhitungan **gradien** dari fungsi biaya terhadap parameter $ \\theta $ dan memperbarui parameter tersebut dengan langkah-langkah kecil, bergantung pada **learning rate**.\n",
    "\n",
    "Dengan menggunakan gradient descent, kita dapat menemukan parameter optimal untuk model logistic regression yang memberikan prediksi terbaik berdasarkan data latih."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
