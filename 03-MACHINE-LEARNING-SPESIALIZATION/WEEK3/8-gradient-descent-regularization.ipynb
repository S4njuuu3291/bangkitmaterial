{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengimplementasikan **Gradient Descent** pada **Regresi Linier dengan Regularisasi**\n",
    "\n",
    "Dalam video kali ini, kita akan mempelajari bagaimana cara membuat **gradient descent** bekerja dengan **regresi linier yang telah di-regularisasi**. Mari kita mulai.\n",
    "\n",
    "---\n",
    "\n",
    "## Fungsi Biaya pada Regresi Linier dengan Regularisasi\n",
    "\n",
    "Fungsi biaya untuk **regresi linier dengan regularisasi** terdiri dari dua bagian:\n",
    "1. **Fungsi biaya biasa**: Ini adalah fungsi kesalahan kuadrat (squared error) yang biasa kita gunakan.\n",
    "2. **Term regularisasi**: Di sini, kita menambahkan komponen regularisasi untuk mengurangi kompleksitas model.\n",
    "\n",
    "### Fungsi Biaya dengan Regularisasi\n",
    "\n",
    "Fungsi biaya yang sudah di-regularisasi adalah sebagai berikut:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "- $ h(x^{(i)}) $ adalah prediksi model untuk data ke-i.\n",
    "- $ y^{(i)} $ adalah nilai sebenarnya dari data ke-i.\n",
    "- $ \\lambda $ adalah parameter regularisasi yang mengontrol seberapa besar pengaruh regularisasi terhadap model.\n",
    "- $ w_j $ adalah bobot atau parameter untuk fitur ke-j.\n",
    "- $ m $ adalah jumlah contoh dalam dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Algoritma **Gradient Descent** dengan Regularisasi\n",
    "\n",
    "### Update Parameter\n",
    "\n",
    "Untuk algoritma **gradient descent** dengan regularisasi, kita akan memperbarui parameter $ w_j $ dan $ b $ menggunakan rumus yang mirip dengan yang sudah kita gunakan pada regresi linier biasa. Namun, ada perubahan kecil pada bagian gradien terhadap $ w_j $ karena adanya tambahan term regularisasi.\n",
    "\n",
    "### Gradien Terhadap $ w_j $\n",
    "\n",
    "Gradien terhadap $ w_j $ pada **regresi linier dengan regularisasi** adalah:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "Gradien terhadap $ b $ tetap sama, karena **regularisasi tidak mempengaruhi bias** $ b $. Oleh karena itu, update untuk $ b $ tetap menggunakan rumus yang sama seperti sebelumnya.\n",
    "\n",
    "### Update Bobot ($ w_j $) dan Bias ($ b $)\n",
    "\n",
    "Untuk **update** parameter dengan **gradient descent** yang sudah di-regularisasi, kita menggunakan rumus:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} w_j \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "- $ \\alpha $ adalah **learning rate**, yang mengontrol seberapa besar perubahan yang kita lakukan pada setiap iterasi.\n",
    "- $ \\lambda $ adalah **parameter regularisasi**.\n",
    "\n",
    "### Proses Pembaruan\n",
    "\n",
    "Perhatikan bahwa untuk **update** $ w_j $, ada dua komponen:\n",
    "1. **Komponen biasa** yang sama dengan regresi linier tanpa regularisasi.\n",
    "2. **Komponen regularisasi** yang memperkenalkan faktor $ \\lambda $ untuk menekan nilai parameter $ w_j $, yang membantu menghindari overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Memahami Regularisasi dalam Gradient Descent\n",
    "\n",
    "### Pengaruh Regularisasi pada Pembaruan\n",
    "\n",
    "Jika kita memeriksa update untuk $ w_j $, kita bisa menulisnya ulang sebagai:\n",
    "\n",
    "$$\n",
    "w_j := w_j \\left( 1 - \\frac{\\alpha \\lambda}{m} \\right) - \\frac{\\alpha}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Pada bagian ini, kita mengalikan $ w_j $ dengan **faktor pengurangan** $ 1 - \\frac{\\alpha \\lambda}{m} $. Dengan demikian, setiap kali kita melakukan iterasi, nilai $ w_j $ akan sedikit menyusut, yang mengurangi efek dari parameter yang besar. Inilah yang disebut dengan **penyusutan parameter** atau **shrinkage**.\n",
    "\n",
    "### Apa yang Terjadi Setiap Iterasi?\n",
    "\n",
    "Jika $ \\lambda $ adalah angka kecil, seperti 1, dan $ m $ adalah ukuran dataset, misalnya 50, maka faktor pengurangan menjadi sangat kecil (misalnya, $ 0.9998 $). Jadi, setiap kali kita melakukan pembaruan, kita mengalikan $ w_j $ dengan angka sedikit lebih kecil dari 1. Ini menyebabkan nilai $ w_j $ sedikit berkurang, yang mengarah pada pengurangan overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Mengapa Regularisasi Itu Berguna?\n",
    "\n",
    "Regularisasi membantu mencegah model menjadi terlalu kompleks dan overfitted, terutama saat kita memiliki banyak fitur dan sedikit data pelatihan. Dengan melakukan penyusutan pada parameter, kita memastikan bahwa model tidak terlalu bergantung pada fitur tertentu yang bisa jadi hanya relevan untuk data pelatihan.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivasi Gradien untuk Regularisasi\n",
    "\n",
    "Untuk pemahaman lebih lanjut, kita dapat melihat bagaimana turunan dari fungsi biaya dihitung. Pada dasarnya, gradien dari fungsi biaya dengan regularisasi adalah hasil dari turunan dari dua bagian:\n",
    "\n",
    "1. **Bagian kesalahan kuadrat (squared error)**.\n",
    "2. **Bagian regularisasi** yang menambahkan penalti pada besar parameter $ w_j $.\n",
    "\n",
    "Secara matematis, derivatif untuk $ w_j $ dengan regularisasi adalah:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Ringkasan\n",
    "\n",
    "1. **Regresi Linier dengan Regularisasi** menggunakan fungsi biaya yang terdiri dari dua bagian: fungsi kesalahan kuadrat dan penalti untuk besar parameter.\n",
    "2. **Gradient Descent** digunakan untuk memperbarui parameter model dengan menambahkan komponen regularisasi yang mengurangi besar parameter $ w_j $ di setiap iterasi.\n",
    "3. Regularisasi menghindari overfitting dengan **menyusutkan** nilai parameter secara bertahap.\n",
    "\n",
    "Dengan memahami **regresi linier dengan regularisasi**, Anda dapat mengurangi overfitting, terutama saat memiliki banyak fitur dan dataset yang terbatas.\n",
    "\n",
    "---\n",
    "\n",
    "## Langkah Selanjutnya\n",
    "\n",
    "Di video berikutnya, kita akan **menerapkan teknik regularisasi** ini pada **logistic regression** untuk mencegah overfitting pada model klasifikasi. Mari kita lihat bagaimana hal itu diterapkan pada logistic regression di video selanjutnya."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
