{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cara Mengetahui Apakah Gradient Descent Sudah Konvergen\n",
    "\n",
    "Saat Anda menjalankan **gradient descent**, bagaimana Anda tahu bahwa proses tersebut sudah **konvergen** dan mendekati **minimum global** dari fungsi biaya (**cost function**)? Mengetahui kapan **gradient descent** sudah konvergen sangat penting agar proses pelatihan berhenti pada waktu yang tepat, menghindari pemborosan waktu dan sumber daya.\n",
    "\n",
    "Di artikel ini, kita akan membahas cara memahami konvergensi pada **gradient descent** secara sederhana, serta bagaimana memonitor dan memverifikasi bahwa proses konvergensi telah tercapai.\n",
    "\n",
    "---\n",
    "\n",
    "### **Apa Itu Konvergensi dalam Gradient Descent?**\n",
    "\n",
    "Konvergensi berarti bahwa **gradient descent** sudah menemukan titik yang hampir optimal (atau **minimum global**) dari fungsi biaya **J**. Titik ini adalah tempat di mana fungsi biaya tidak akan berkurang lebih banyak lagi, atau berkurang sangat sedikit. Setelah konvergensi tercapai, model kita tidak akan belajar lebih banyak lagi, dan kita bisa berhenti melatihnya.\n",
    "\n",
    "Namun, bagaimana kita tahu kapan itu terjadi? Berikut adalah cara-cara untuk mengetahuinya.\n",
    "\n",
    "---\n",
    "\n",
    "### **Aturan Dasar Gradient Descent**\n",
    "\n",
    "Untuk memahami konvergensi, pertama-tama kita harus ingat kembali prinsip dasar dari **gradient descent**:\n",
    "\n",
    "- **Tujuan utama**: Menggunakan **gradient descent** untuk menemukan parameter **w** dan **b** (misalnya, bobot dan bias dalam regresi linear) yang **meminimalkan** fungsi biaya **J**.\n",
    "- **Proses iteratif**: Pada setiap iterasi, kita memperbarui **w** dan **b** berdasarkan gradien dari fungsi biaya.\n",
    "\n",
    "Namun, hal yang paling penting dalam **gradient descent** adalah pemilihan **learning rate** (disebut juga **α** atau **Alpha**). **Learning rate** ini menentukan seberapa besar langkah yang diambil untuk memperbarui parameter pada setiap iterasi. Jika learning rate terlalu besar, kita bisa melangkah terlalu jauh dan melewatkan minimum global. Jika terlalu kecil, prosesnya bisa sangat lambat.\n",
    "\n",
    "---\n",
    "\n",
    "### **Memplot Fungsi Biaya (Cost Function)**\n",
    "\n",
    "Salah satu cara untuk memantau apakah **gradient descent** konvergen adalah dengan memplot fungsi biaya (**cost function J**) pada setiap iterasi.\n",
    "\n",
    "- **Sumbu horizontal (x-axis)**: Menunjukkan jumlah **iterasi** yang telah dilakukan selama proses pelatihan.\n",
    "- **Sumbu vertikal (y-axis)**: Menunjukkan nilai **cost function J** pada setiap iterasi.\n",
    "\n",
    "Sebagai contoh, kita bisa memplot nilai **J** setiap kali parameter **w** dan **b** diperbarui. Ini akan membentuk yang disebut sebagai **learning curve**.\n",
    "\n",
    "**Learning curve** menunjukkan bagaimana **cost function** berubah seiring berjalannya iterasi. Jika **J** terus menurun, itu menandakan bahwa **gradient descent** berjalan dengan baik.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mengidentifikasi Kurva Pembelajaran (Learning Curve)**\n",
    "\n",
    "**Learning curve** adalah grafik yang menggambarkan perubahan **cost function** (J) seiring bertambahnya iterasi. Berikut cara mengenali **learning curve** yang baik:\n",
    "\n",
    "- **Sumbu horizontal (x)**: Jumlah iterasi **gradient descent**.\n",
    "- **Sumbu vertikal (y)**: Nilai **cost function J** yang dihitung pada iterasi tersebut.\n",
    "\n",
    "Pada awalnya, **J** biasanya akan menurun tajam, karena model belajar dengan cepat. Namun, seiring berjalannya waktu, penurunan ini akan semakin melambat. Ini adalah tanda bahwa model sedang mendekati nilai **minimum** dari fungsi biaya.\n",
    "\n",
    "#### Tanda-Tanda Gradient Descent Berjalan Dengan Baik:\n",
    "- **Cost function J menurun** pada setiap iterasi. Ini menunjukkan bahwa model Anda sedang menuju ke titik minimum.\n",
    "- Jika **J** justru meningkat, kemungkinan ada masalah, seperti **learning rate** yang terlalu besar atau kesalahan dalam implementasi kode.\n",
    "\n",
    "---\n",
    "\n",
    "### **Konvergensi: Kapan Proses Berhenti?**\n",
    "\n",
    "Bagaimana kita tahu jika **gradient descent** sudah mencapai konvergensi? Salah satu indikasinya adalah ketika grafik **learning curve** mulai **mendatar** atau tidak menunjukkan penurunan yang signifikan lagi.\n",
    "\n",
    "#### Tanda-Tanda Konvergensi:\n",
    "- **Learning curve mendatar**: Setelah sejumlah iterasi, kurva mulai mendekati garis datar. Ini menandakan bahwa **cost function J** tidak lagi menurun secara signifikan.\n",
    "- **Penurunan J sangat kecil**: Jika penurunan **J** pada iterasi selanjutnya sangat kecil, artinya model sudah mendekati titik minimum.\n",
    "\n",
    "Namun, jumlah iterasi yang diperlukan untuk konvergensi bisa berbeda-beda, tergantung pada banyak faktor, termasuk:\n",
    "- **Masalah yang sedang diselesaikan**\n",
    "- **Learning rate**\n",
    "- **Kualitas data** dan **fitur yang digunakan**\n",
    "\n",
    "Sebagai contoh, pada satu aplikasi, gradient descent mungkin hanya membutuhkan 30 iterasi untuk mencapai konvergensi. Namun, pada aplikasi lain yang lebih kompleks, bisa membutuhkan ribuan iterasi.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pengujian Konvergensi Otomatis**\n",
    "\n",
    "Selain menggunakan **learning curve**, Anda bisa menerapkan **pengujian konvergensi otomatis** untuk mengetahui kapan **gradient descent** harus berhenti.\n",
    "\n",
    "- **Threshold epsilon (𝜖)**: Biasanya, kita menetapkan **epsilon** sebagai nilai yang sangat kecil (misalnya, 0.001 atau 10^-3). Jika penurunan **J** antara iterasi berturut-turut lebih kecil dari **epsilon**, kita bisa menghentikan proses pelatihan karena itu menunjukkan bahwa penurunan biaya sudah sangat kecil.\n",
    "  \n",
    "**Contoh**: Jika penurunan biaya antara iterasi 100 dan 101 hanya 0.0001, dan kita telah menetapkan epsilon = 0.001, maka kita bisa menghentikan proses pelatihan karena perubahan sudah sangat kecil dan model kita sudah hampir konvergen.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mengapa Visualisasi Lebih Baik?**\n",
    "\n",
    "Meskipun pengujian otomatis dengan **epsilon** sangat berguna, saya pribadi lebih suka **melihat grafik learning curve** secara visual. Grafik memberikan gambaran yang lebih jelas tentang bagaimana proses pelatihan berjalan, dan bisa membantu Anda mendeteksi masalah seperti:\n",
    "- **Learning rate yang terlalu besar** (menyebabkan fluktuasi besar pada nilai **J**).\n",
    "- **Model overfitting** jika biaya mulai naik kembali setelah beberapa iterasi.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kesimpulan: Mengapa Memantau Konvergensi Itu Penting?**\n",
    "\n",
    "Memantau **learning curve** dan memahami cara **gradient descent** berkonvergensi sangat penting untuk mendapatkan hasil terbaik dari model Anda. Dengan cara ini, Anda bisa memastikan bahwa proses pelatihan berhenti pada waktu yang tepat, ketika model Anda telah menemukan solusi yang optimal.\n",
    "\n",
    "Beberapa hal yang perlu Anda ingat:\n",
    "- **Learning curve** akan membantu Anda mengetahui apakah **J** terus menurun.\n",
    "- Jika **J** mendatar atau penurunannya sangat kecil, maka **gradient descent** sudah hampir konvergen.\n",
    "- Anda bisa menggunakan **epsilon** sebagai pengujian otomatis, tetapi **grafik** memberikan gambaran yang lebih jelas.\n",
    "- Pemilihan **learning rate** yang tepat sangat penting untuk konvergensi yang efisien.\n",
    "\n",
    "Dengan memahami cara memantau konvergensi, Anda bisa memastikan bahwa model yang Anda bangun berjalan dengan efektif dan efisien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
