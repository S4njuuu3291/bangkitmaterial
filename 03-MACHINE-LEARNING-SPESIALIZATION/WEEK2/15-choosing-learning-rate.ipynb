{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menentukan Learning Rate yang Tepat\n",
    "\n",
    "Pemilihan **learning rate** yang tepat sangat penting untuk keberhasilan dan efisiensi algoritma pembelajaran mesin, khususnya dalam **gradient descent**. Learning rate yang terlalu kecil dapat memperlambat proses pelatihan, sedangkan learning rate yang terlalu besar dapat menyebabkan model tidak konvergen atau bahkan gagal mencapai solusi optimal.\n",
    "\n",
    "---\n",
    "\n",
    "### **Apa Itu Learning Rate?**\n",
    "\n",
    "**Learning rate (α)** adalah parameter yang mengatur seberapa besar langkah yang diambil dalam setiap iterasi ketika memperbarui parameter model (misalnya, **w** dan **b** dalam regresi linear). Pemilihan learning rate yang tepat sangat memengaruhi kecepatan dan kualitas konvergensi model. \n",
    "\n",
    "- Jika **α** terlalu kecil, model akan sangat lambat dalam mencapai konvergensi.\n",
    "- Jika **α** terlalu besar, model mungkin **melewati** minimum global dan tidak pernah konvergen.\n",
    "\n",
    "---\n",
    "\n",
    "### **Indikator Kesalahan pada Gradient Descent**\n",
    "\n",
    "Ketika Anda memplot **fungsi biaya** (**cost function J**) selama beberapa iterasi, ada dua indikasi utama yang menunjukkan bahwa **gradient descent** tidak berjalan dengan baik:\n",
    "\n",
    "1. **Fungsi biaya naik turun**: Jika Anda melihat **fungsi biaya (J)** naik dan turun tanpa arah yang jelas, ini menunjukkan masalah pada **learning rate** atau ada **bug dalam kode**.\n",
    "2. **Fungsi biaya terus meningkat**: Jika **J** terus meningkat dari iterasi ke iterasi, ini bisa menandakan bahwa learning rate terlalu besar atau ada kesalahan dalam pembaruan parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ilustrasi Overshoot**\n",
    "\n",
    "Penting untuk memahami apa yang terjadi jika **learning rate terlalu besar**:\n",
    "\n",
    "- **Overshoot** adalah situasi di mana pembaruan parameter **terlalu besar**, sehingga parameter **melewati** titik minimum dan malah menjauh darinya.\n",
    "- **Prosesnya**: Setiap kali **gradient descent** mengupdate parameter, langkah yang besar bisa menyebabkan model berbalik arah, menjauhi **minimum global**.\n",
    "\n",
    "Sebagai contoh:\n",
    "- Jika sumbu vertikal adalah **fungsi biaya (J)**, dan sumbu horizontal adalah **parameter** (misalnya, **w_1**), langkah yang terlalu besar dalam memperbarui **w_1** bisa membuatnya **melewati** titik minimum, dan **J** justru meningkat.\n",
    "\n",
    "#### Solusi:\n",
    "- Jika Anda menghadapi overshoot, solusinya adalah menggunakan **learning rate** yang lebih kecil. Dengan langkah yang lebih kecil, gradient descent akan melakukan pembaruan yang lebih halus, semakin mendekati minimum global.\n",
    "\n",
    "---\n",
    "\n",
    "### **Peningkatan Biaya Secara Konsisten**\n",
    "\n",
    "Tanda lain bahwa **learning rate terlalu besar** adalah ketika **fungsi biaya** (J) terus **naik** setelah setiap iterasi, alih-alih menurun.\n",
    "\n",
    "#### Contoh Kode yang Salah\n",
    "\n",
    "Jika ada **bug dalam kode**, misalnya pembaruan parameter yang salah, Anda mungkin akan melihat peningkatan biaya meski sudah mencoba learning rate yang lebih kecil. Contoh kesalahan:\n",
    "\n",
    "```python\n",
    "w_1 = w_1 + Alpha * turunan  # Salah\n",
    "```\n",
    "\n",
    "Seharusnya pembaruan parameter dilakukan dengan **mengurangi** nilai **w_1**, bukan menambahkannya. Yang benar adalah:\n",
    "\n",
    "```python\n",
    "w_1 = w_1 - Alpha * turunan  # Benar\n",
    "```\n",
    "\n",
    "Jika Anda menambahkannya, maka **fungsi biaya J** justru akan semakin jauh dari minimum, dan **gradient descent** tidak akan konvergen.\n",
    "\n",
    "---\n",
    "\n",
    "### **Debugging Gradient Descent**\n",
    "\n",
    "Jika Anda melihat **fungsi biaya** tidak menurun meski learning rate kecil, bisa jadi ada bug dalam kode. Salah satu cara untuk mendeteksi kesalahan adalah dengan **menggunakan learning rate yang sangat kecil** (misalnya, 0.0001). Jika **J** masih tidak turun meski learning rate sangat kecil, maka kemungkinan besar ada masalah dengan implementasi kode Anda.\n",
    "\n",
    "---\n",
    "\n",
    "### **Trade-off dalam Pemilihan Learning Rate**\n",
    "\n",
    "Memilih **learning rate** bukanlah hal yang sederhana. Jika **α** terlalu kecil, proses pelatihan akan berjalan sangat lambat, membutuhkan banyak iterasi untuk mencapai konvergensi. Sebaliknya, jika **α** terlalu besar, kita mungkin tidak bisa mencapai minimum global dan proses pelatihan bisa gagal.\n",
    "\n",
    "#### Apa yang Harus Diperhatikan?\n",
    "- **Learning rate yang terlalu kecil**: Proses pelatihan akan berjalan lambat dan membutuhkan banyak iterasi.\n",
    "- **Learning rate yang terlalu besar**: Proses pelatihan bisa gagal, dengan **J** tidak turun atau malah naik.\n",
    "\n",
    "Oleh karena itu, Anda perlu mencari nilai **α** yang tepat: cukup besar agar proses pelatihan cepat, tetapi cukup kecil untuk memastikan konvergensi yang stabil.\n",
    "\n",
    "---\n",
    "\n",
    "### **Teknik Pemilihan Learning Rate**\n",
    "\n",
    "Untuk memilih **learning rate** yang tepat, Anda bisa menggunakan pendekatan eksperimen, mencoba berbagai nilai untuk **α** dan melihat bagaimana pengaruhnya terhadap **fungsi biaya (J)**.\n",
    "\n",
    "#### Langkah-Langkah:\n",
    "1. **Mulai dengan nilai kecil**: Cobalah learning rate kecil, misalnya **0.001**.\n",
    "2. **Eksperimen dengan nilai yang lebih besar**: Cobalah nilai yang lebih besar, seperti **0.01**, **0.1**, dan seterusnya.\n",
    "3. **Plot fungsi biaya (J)**: Setiap kali Anda mencoba nilai learning rate yang berbeda, jalankan gradient descent beberapa iterasi dan plot **J** terhadap jumlah iterasi.\n",
    "4. **Pilih learning rate terbaik**: Pilih nilai **α** yang membuat **J** menurun secara konsisten dan cepat.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pencarian Optimal Learning Rate**\n",
    "\n",
    "Untuk memilih **learning rate** secara optimal, cobalah teknik pencarian bertahap. Berikut adalah pendekatan yang bisa Anda coba:\n",
    "\n",
    "1. Mulai dengan **α = 0.001**.\n",
    "2. Tingkatkan menjadi **α = 0.003**, kemudian **α = 0.01**, dan seterusnya.\n",
    "3. Perhatikan bagaimana **fungsi biaya J** bereaksi pada setiap perubahan learning rate. Jika **J** turun dengan konsisten, Anda berada di jalur yang benar.\n",
    "4. Setelah menemukan nilai **α** yang terlalu kecil dan terlalu besar, pilih nilai **α** terbesar yang masih memberikan **konvergensi** yang stabil.\n",
    "\n",
    "Dengan cara ini, Anda bisa menemukan learning rate yang optimal yang memberi hasil terbaik tanpa memperlambat atau melompat-lompat dari titik minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kesimpulan**\n",
    "\n",
    "Pemilihan **learning rate** yang tepat sangat penting untuk membuat **gradient descent** berjalan dengan efisien. Jika learning rate terlalu besar, model bisa melewati minimum, sementara jika terlalu kecil, prosesnya akan sangat lambat.\n",
    "\n",
    "Berikut adalah beberapa poin penting:\n",
    "- **Overshoot** terjadi jika learning rate terlalu besar, dan Anda bisa memperbaikinya dengan mengecilkan **α**.\n",
    "- Jika **fungsi biaya** terus meningkat, itu bisa menandakan kesalahan dalam kode atau learning rate yang terlalu besar.\n",
    "- **Eksperimen dengan berbagai nilai α** untuk menemukan yang terbaik.\n",
    "- Pencarian bertahap dalam memilih learning rate bisa membantu menemukan nilai yang optimal dengan lebih cepat.\n",
    "\n",
    "Dengan teknik-teknik ini, Anda dapat menemukan **learning rate** yang paling cocok untuk model Anda, dan memastikan pelatihan yang lebih cepat dan lebih stabil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
