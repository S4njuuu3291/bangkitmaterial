{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Gradient Descent untuk Regresi Linear Berganda dengan Vektorisasi**\n",
    "\n",
    "## **Pendahuluan**\n",
    "\n",
    "Pada pembelajaran ini, kita akan menerapkan **Gradient Descent** untuk regresi linear berganda menggunakan **vektorisasi**. Sebelum melangkah lebih jauh, pastikan Anda telah mempelajari dasar-dasar tentang **regresi linear berganda**, **gradient descent**, dan **vektorisasi**. \n",
    "\n",
    "### **Model Regresi Linear Berganda**\n",
    "\n",
    "Dalam regresi linear berganda, kita mencoba untuk memodelkan hubungan antara **variabel independen** (fitur) dengan **variabel dependen** (target). Jika ada $ n $ fitur, model regresi linear dapat dituliskan sebagai berikut:\n",
    "\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ x_1, x_2, \\dots, x_n $ adalah fitur-fitur (input),\n",
    "- $ w_1, w_2, \\dots, w_n $ adalah bobot (parameter) yang perlu kita tentukan,\n",
    "- $ b $ adalah **bias** (intercept),\n",
    "- $ y $ adalah prediksi (output).\n",
    "\n",
    "Namun, penulisan ini bisa lebih efisien dengan menggunakan notasi vektor. Kita bisa menggabungkan semua fitur $ x $ dan bobot $ w $ ke dalam vektor, sehingga model regresi linear berganda dapat ditulis sebagai:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ w = [w_1, w_2, \\dots, w_n] $ adalah vektor bobot,\n",
    "- $ x = [x_1, x_2, \\dots, x_n] $ adalah vektor fitur,\n",
    "- $ w \\cdot x $ adalah hasil perkalian titik antara vektor $ w $ dan $ x $.\n",
    "\n",
    "Dengan notasi ini, kita bisa memproses data secara lebih efisien, terutama ketika jumlah fitur sangat besar.\n",
    "\n",
    "---\n",
    "\n",
    "### **Fungsi Biaya (Cost Function) untuk Regresi Linear**\n",
    "\n",
    "Tujuan dari regresi linear adalah meminimalkan kesalahan antara nilai yang diprediksi ($ \\hat{y} $) dan nilai yang sebenarnya ($ y $). Untuk itu, kita menggunakan **Mean Squared Error (MSE)** sebagai fungsi biaya, yang dapat dituliskan sebagai:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ m $ adalah jumlah contoh data,\n",
    "- $ \\hat{y}^{(i)} = w \\cdot x^{(i)} + b $ adalah prediksi untuk contoh data ke-i,\n",
    "- $ y^{(i)} $ adalah nilai aktual untuk contoh data ke-i.\n",
    "\n",
    "Kenapa ada faktor $ \\frac{1}{2} $? Faktor ini memudahkan perhitungan turunan saat melakukan **gradient descent**, karena turunan dari kuadrat $ (a^2) $ akan menghasilkan $ 2a $, sehingga faktor 2 akan saling membatalkan.\n",
    "\n",
    "---\n",
    "\n",
    "### **Update Parameter dengan Gradient Descent**\n",
    "\n",
    "Gradient descent adalah algoritma iteratif yang digunakan untuk meminimalkan fungsi biaya $ J(w, b) $. Setiap iterasi, kita menghitung **gradien** dari fungsi biaya terhadap parameter $ w $ dan $ b $, dan kemudian meng-update parameter-parameter tersebut dalam arah yang mengurangi biaya. Update parameter dilakukan dengan rumus berikut:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ \\alpha $ adalah **learning rate**, yang mengontrol seberapa besar langkah yang diambil pada setiap iterasi,\n",
    "- $ \\frac{\\partial J}{\\partial w_j} $ dan $ \\frac{\\partial J}{\\partial b} $ adalah turunan dari fungsi biaya terhadap parameter $ w_j $ dan $ b $.\n",
    "\n",
    "#### **Turunan Fungsi Biaya**\n",
    "\n",
    "Turunan fungsi biaya $ J(w, b) $ terhadap $ w_j $ dan $ b $ dapat dihitung sebagai berikut:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "Dengan menggunakan turunan ini, kita dapat mengupdate $ w_j $ dan $ b $ pada setiap iterasi gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Vektorisasi untuk Efisiensi Komputasi**\n",
    "\n",
    "Alih-alih melakukan perhitungan secara manual untuk setiap parameter $ w_j $, kita dapat menggunakan **vektorisasi** untuk memproses semua parameter secara bersamaan. Dalam notasi vektor, update parameter bisa ditulis sebagai:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\frac{1}{m} X^T (Xw + b - y)\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (Xw + b - y)\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ X $ adalah matriks fitur dengan dimensi $ m \\times n $,\n",
    "- $ y $ adalah vektor target dengan dimensi $ m \\times 1 $,\n",
    "- $ Xw + b $ adalah prediksi untuk semua contoh data,\n",
    "- $ \\alpha $ adalah learning rate.\n",
    "\n",
    "Vektorisasi ini mempercepat komputasi, terutama pada dataset yang besar, karena kita menghindari penggunaan perulangan untuk setiap parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Alternatif: Normal Equation**\n",
    "\n",
    "Selain menggunakan gradient descent, kita juga dapat menggunakan **normal equation** untuk menghitung parameter $ w $ secara langsung tanpa iterasi. Normal equation adalah solusi analitik untuk regresi linear berganda yang diturunkan dari kondisi optimal.\n",
    "\n",
    "Normal equation untuk mencari $ w $ adalah:\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Namun, normal equation memiliki keterbatasan. Jika jumlah fitur $ n $ sangat besar, maka komputasi untuk invers matriks $ (X^T X)^{-1} $ menjadi sangat mahal, sehingga lebih disarankan untuk menggunakan gradient descent pada masalah dengan jumlah fitur besar.\n",
    "\n",
    "---\n",
    "\n",
    "## **Scaling Fitur untuk Mempercepat Gradient Descent**\n",
    "\n",
    "### **Pentingnya Scaling Fitur**\n",
    "\n",
    "Salah satu masalah dalam penerapan gradient descent adalah ketika fitur-fitur memiliki skala yang sangat berbeda. Misalnya, dalam prediksi harga rumah, fitur-fitur seperti **ukuran rumah** (dalam kaki persegi) dan **jumlah kamar tidur** memiliki rentang nilai yang sangat berbeda:\n",
    "\n",
    "- Ukuran rumah ($ x_1 $) bisa berkisar antara 300 hingga 2000 kaki persegi.\n",
    "- Jumlah kamar tidur ($ x_2 $) bisa berkisar antara 0 hingga 5.\n",
    "\n",
    "Jika kita menjalankan gradient descent tanpa scaling, fitur yang memiliki rentang nilai lebih besar (misalnya $ x_1 $) akan mendominasi perhitungan gradien, sementara fitur yang memiliki rentang nilai kecil (misalnya $ x_2 $) akan memiliki dampak yang lebih kecil pada perhitungan parameter. Hal ini dapat memperlambat konvergensi algoritma.\n",
    "\n",
    "### **Proses Scaling Fitur**\n",
    "\n",
    "Untuk mengatasi masalah ini, kita dapat **menormalisasi** atau **menskalakan** fitur-fitur sehingga mereka berada dalam rentang yang sama. Salah satu metode yang paling umum digunakan adalah **min-max scaling**, di mana setiap fitur $ x_j $ dikurangi dengan nilai minimum dan dibagi dengan rentang (selisih antara nilai maksimum dan minimum):\n",
    "\n",
    "$$\n",
    "x'_j = \\frac{x_j - \\min(x_j)}{\\max(x_j) - \\min(x_j)}\n",
    "$$\n",
    "\n",
    "Alternatif lain adalah **standardisasi**, di mana setiap fitur $ x_j $ dikurangi dengan rata-rata dan dibagi dengan standar deviasi:\n",
    "\n",
    "$$\n",
    "x'_j = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "di mana:\n",
    "- $ \\mu_j $ adalah rata-rata fitur $ x_j $,\n",
    "- $ \\sigma_j $ adalah standar deviasi fitur $ x_j $.\n",
    "\n",
    "### **Efek Scaling pada Gradient Descent**\n",
    "\n",
    "Dengan fitur yang sudah diskalakan, proses gradient descent menjadi lebih stabil dan lebih cepat. Misalnya, dalam plot kontur fungsi biaya, tanpa scaling, gradient descent mungkin akan bergerak dalam lintasan yang berbentuk zig-zag, sementara dengan scaling, jalur menuju minimum global akan lebih lurus dan lebih cepat tercapai.\n",
    "\n",
    "### **Ringkasan**\n",
    "\n",
    "Scaling fitur sangat penting untuk mempercepat proses konvergensi gradient descent. Dengan menskalakan fitur, kita memastikan bahwa setiap fitur berkontribusi secara seimbang dalam perhitungan gradien dan membantu algoritma mencapai solusi optimal lebih cepat.\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementasi di Python**\n",
    "\n",
    "Mari kita lihat bagaimana kita dapat mengimplementasikan gradient descent untuk regresi linear berganda di Python menggunakan pust\n",
    "\n",
    "aka NumPy.\n",
    "\n",
    "### **Langkah 1: Persiapkan Data**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Data contoh (X adalah fitur, y adalah target)\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([5, 11, 17, 23])\n",
    "\n",
    "# Menambahkan kolom 1 untuk bias (intercept)\n",
    "X = np.c_[np.ones(X.shape[0]), X]  # Menambahkan kolom 1 untuk bias\n",
    "```\n",
    "\n",
    "### **Langkah 2: Inisialisasi Parameter**\n",
    "\n",
    "```python\n",
    "# Inisialisasi parameter\n",
    "w = np.zeros(X.shape[1])  # Vektor bobot, satu untuk setiap fitur + bias\n",
    "alpha = 0.01  # Learning rate\n",
    "iterations = 1000  # Jumlah iterasi\n",
    "m = len(y)  # Jumlah data\n",
    "```\n",
    "\n",
    "### **Langkah 3: Implementasi Gradient Descent**\n",
    "\n",
    "```python\n",
    "# Implementasi Gradient Descent\n",
    "for i in range(iterations):\n",
    "    predictions = X.dot(w)  # Prediksi\n",
    "    errors = predictions - y  # Kesalahan\n",
    "    gradient = (1/m) * X.T.dot(errors)  # Gradien\n",
    "    w -= alpha * gradient  # Update parameter\n",
    "```\n",
    "\n",
    "### **Langkah 4: Lihat Hasil**\n",
    "\n",
    "```python\n",
    "print(\"Parameter w yang terlatih:\", w)\n",
    "```\n",
    "\n",
    "Dengan langkah-langkah ini, kita dapat melatih model regresi linear berganda menggunakan gradient descent dengan vektorisasi yang efisien.\n",
    "\n",
    "--- \n",
    "\n",
    "Dengan penjelasan yang lebih rinci ini, semoga Anda dapat memahami bagaimana gradient descent diterapkan pada regresi linear berganda dan pentingnya scaling fitur untuk mempercepat proses konvergensi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metode untuk **Feature Scaling**:\n",
    "\n",
    "1. **Scaling Sederhana:**\n",
    "   - Untuk fitur yang memiliki rentang nilai yang sangat berbeda (misalnya, `x1` memiliki rentang dari 3 hingga 2000), kamu bisa melakukan scaling dengan membagi setiap nilai dengan nilai maksimum dalam rentang tersebut.\n",
    "   - Sebagai contoh, membagi `x1` dengan 2000 akan menskalakan nilainya antara 0,15 hingga 1. Demikian juga, `x2` dapat diskalakan dengan membaginya dengan nilai maksimumnya (5), sehingga skalanya akan berada antara 0 hingga 1.\n",
    "\n",
    "2. **Mean Normalization (Normalisasi Rata-rata):**\n",
    "   - Metode lain adalah **mean normalization**, di mana kamu mengambil nilai fitur asli dan menskalakannya agar fitur tersebut terpusat di sekitar nol, sehingga memiliki nilai negatif dan positif.\n",
    "   - Sebagai contoh, jika `x1` memiliki rentang dari 300 hingga 2000, kamu bisa menghitung rata-rata (atau disebut juga **mean**) dari `x1`, yang kita sebut sebagai `Mu_1`. Misalnya, rata-rata `x1` adalah 600. Kamu bisa mengambil setiap `x1`, mengurangi `Mu_1`, lalu membaginya dengan rentang 2000 dikurangi 300. Hasilnya, nilai `x1` yang sudah dinormalisasi akan berkisar dari -0,18 hingga 0,82.\n",
    "   - Untuk fitur `x2`, jika memiliki rentang dari 0 hingga 5, kamu bisa menghitung rata-rata `Mu_2`, lalu mengambil setiap `x2`, menguranginya dengan `Mu_2`, dan membaginya dengan 5 - 0. Hasilnya, nilai `x2` yang sudah dinormalisasi akan berkisar dari -0,46 hingga 0,54.\n",
    "\n",
    "3. **Z-Score Normalization (Normalisasi Z-Score):**\n",
    "   - Metode lain adalah **Z-score normalization**, di mana kamu menghitung rata-rata (`Mu`) dan standar deviasi (`Sigma`) untuk setiap fitur.\n",
    "   - Sebagai contoh, jika fitur `x1` memiliki standar deviasi 450 dan rata-rata 600, maka untuk melakukan **Z-score normalization**, ambil setiap `x1`, kurangi dengan `Mu_1`, lalu bagi dengan `Sigma_1`. Hasilnya, nilai `x1` yang sudah dinormalisasi dengan Z-score bisa berkisar dari -0,67 hingga 3,1.\n",
    "   - Demikian juga, untuk fitur `x2`, jika standar deviasinya adalah 1,4 dan rata-ratanya 2,3, kamu bisa menghitung `x2 - Mu_2` dan membaginya dengan `Sigma_2`. Dalam kasus ini, hasilnya berkisar antara -1,6 hingga 1,9.\n",
    "\n",
    "### Panduan Umum:\n",
    "   - Sebagai aturan umum, ketika melakukan feature scaling, kamu mungkin ingin menskalakan fitur-fitur agar rentangnya berada sekitar -1 hingga 1 untuk setiap fitur `x`. Namun, rentang ini tidak harus selalu tepat. Jika rentang fiturnya antara -3 hingga 3, atau -0,3 hingga 0,3, itu masih bisa diterima.\n",
    "   - Jika fitur `x3` memiliki rentang yang jauh berbeda, misalnya dari -100 hingga 100, maka sebaiknya fitur ini diskalakan agar rentangnya lebih mendekati -1 hingga 1.\n",
    "   - Jika fitur `x4` memiliki nilai yang sangat kecil, seperti antara -0,001 dan 0,001, maka disarankan juga untuk melakukan scaling.\n",
    "   - Contoh lainnya, jika fitur `x5` (misalnya suhu tubuh pasien di rumah sakit) memiliki rentang dari 98,6 hingga 105 derajat Fahrenheit, maka nilai ini cukup besar dibandingkan fitur yang sudah diskalakan lainnya. Melakukan scaling dalam kasus ini bisa mempercepat algoritma gradient descent.\n",
    "\n",
    "### Kesimpulan:\n",
    "   - Hampir tidak ada kerugian dalam melakukan **feature scaling**. Jika ragu, lakukan saja scaling untuk memastikan model berjalan dengan baik.\n",
    "   - Scaling fitur dapat membantu mempercepat algoritma seperti **gradient descent**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
