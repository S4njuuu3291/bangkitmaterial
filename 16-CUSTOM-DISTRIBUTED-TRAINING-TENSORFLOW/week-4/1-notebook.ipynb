{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategi Distribusi dalam Pelatihan Model\n",
    "\n",
    "Dalam membahas berbagai jenis strategi distribusi, kita akan meninjau semuanya dalam beberapa saat. Penting untuk memikirkan berbagai tempat di mana paralelisme dapat terjadi dan bagaimana kita harus merancang strategi untuk ini.\n",
    "\n",
    "### 1. **Paralelisme pada Platform Perangkat Keras**\n",
    "Ada beberapa pengaturan platform perangkat keras yang dapat digunakan:\n",
    "- **Single Machine Setup**: Di sini, sebuah mesin memiliki banyak perangkat seperti CPU dan satu atau lebih GPU.\n",
    "- **Multiple Machines on a Network**: Beberapa mesin yang terhubung dalam jaringan, dengan masing-masing mesin mungkin memiliki jumlah akselerator yang berbeda atau bahkan tidak ada sama sekali.\n",
    "\n",
    "### 2. **Paralelisme dalam Pendekatan Pelatihan**\n",
    "Ada dua jenis pelatihan paralel melalui *data parallelism*:\n",
    "- **Pelatihan Sinkron (Synchronous Training)**: Semua pekerja melatih potongan data input secara bersamaan. Mereka akan menggabungkan gradien di setiap langkah menggunakan algoritma *all-reduce*. Ini adalah jenis pelatihan yang paling sering digunakan dalam kursus ini.\n",
    "- **Pelatihan Asinkron (Asynchronous Training)**: Semua pekerja melatih data input secara mandiri dan memperbarui variabel secara asinkron. Mereka menyinkronkan model terdistribusi melalui arsitektur *parameter server*.\n",
    "\n",
    "### 3. **Strategi Distribusi TensorFlow**\n",
    "TensorFlow mendukung berbagai strategi untuk membantu Anda melatih model menggunakan skenario yang disebutkan sebelumnya. Beberapa strategi utama yang didukung adalah:\n",
    "\n",
    "#### a. **MirroredStrategy**\n",
    "Strategi ini umumnya digunakan untuk pelatihan terdistribusi pada satu komputer dengan beberapa GPU. \n",
    "- **Cara Kerja**: Model dibuat salinannya di setiap GPU dan variabelnya dipantulkan. Setelah setiap epoch pelatihan, parameter yang dipelajari digabungkan menggunakan algoritma *all-reduce* di antara perangkat.\n",
    "\n",
    "#### b. **TPUStrategy**\n",
    "Mirip dengan *MirroredStrategy*, namun menggunakan core TPU (Tensor Processing Unit) alih-alih GPU. Jika Anda memiliki kluster TPU yang tersedia, Anda bisa menggunakan strategi ini.\n",
    "\n",
    "#### c. **MultiWorkerMirroredStrategy**\n",
    "Digunakan ketika Anda memiliki beberapa mesin dalam jaringan. Setiap mesin ini dapat memiliki jumlah GPU yang berbeda. Strategi ini mereplikasi dan memantulkan model ke setiap pekerja, bukan ke setiap perangkat GPU, dan menggunakan algoritma *all-reduce* berdasarkan pengaturan perangkat keras.\n",
    "\n",
    "#### d. **CentralStorageStrategy**\n",
    "Biasanya digunakan pada satu mesin. Namun, ketika ada banyak GPU pada mesin tersebut, variabel tidak dipantulkan di antara GPU, melainkan disimpan dan diproses oleh CPU. Jika hanya ada satu GPU, GPU digunakan karena variabel yang dipantulkan tidak diperlukan.\n",
    "\n",
    "#### e. **ParameterServerStrategy**\n",
    "Strategi ini menggunakan mesin khusus yang berfungsi sebagai penyimpan data independen, seperti *parameter server*. Variabel model (misalnya bobot, bias, atau filter) disimpan di tempat terpusat. Beberapa mesin dalam jaringan akan menjadi pekerja yang melakukan pelatihan, sementara mesin lainnya adalah parameter server.\n",
    "\n",
    "#### f. **Default Strategy**\n",
    "Merupakan strategi default yang tidak memerlukan perangkat keras atau jaringan khusus. Ini hanya mengizinkan Anda untuk membuat kode distribusi tanpa memerlukan banyak GPU atau mesin.\n",
    "\n",
    "#### g. **OneDeviceStrategy**\n",
    "Strategi ini memfokuskan semua data dan parameter pelatihan pada satu perangkat. Ini cocok untuk prototyping kode pada satu mesin yang nantinya bisa diterapkan pada platform terdistribusi.\n",
    "\n",
    "### 4. **Kesimpulan**\n",
    "Setiap strategi memiliki kegunaan yang berbeda, tergantung pada perangkat keras yang tersedia dan kebutuhan pelatihan model Anda. Dalam kursus ini, kita akan memulai dengan menggunakan *MirroredStrategy* untuk pelatihan distribusi, yang akan memberikan gambaran dasar tentang bagaimana strategi-strategi lain bekerja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
