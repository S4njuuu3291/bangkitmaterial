{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategi Distribusi dalam Pelatihan Model\n",
    "\n",
    "Dalam membahas berbagai jenis strategi distribusi, kita akan meninjau semuanya dalam beberapa saat. Penting untuk memikirkan berbagai tempat di mana paralelisme dapat terjadi dan bagaimana kita harus merancang strategi untuk ini.\n",
    "\n",
    "### 1. **Paralelisme pada Platform Perangkat Keras**\n",
    "Ada beberapa pengaturan platform perangkat keras yang dapat digunakan:\n",
    "- **Single Machine Setup**: Di sini, sebuah mesin memiliki banyak perangkat seperti CPU dan satu atau lebih GPU.\n",
    "- **Multiple Machines on a Network**: Beberapa mesin yang terhubung dalam jaringan, dengan masing-masing mesin mungkin memiliki jumlah akselerator yang berbeda atau bahkan tidak ada sama sekali.\n",
    "\n",
    "### 2. **Paralelisme dalam Pendekatan Pelatihan**\n",
    "Ada dua jenis pelatihan paralel melalui *data parallelism*:\n",
    "- **Pelatihan Sinkron (Synchronous Training)**: Semua pekerja melatih potongan data input secara bersamaan. Mereka akan menggabungkan gradien di setiap langkah menggunakan algoritma *all-reduce*. Ini adalah jenis pelatihan yang paling sering digunakan dalam kursus ini.\n",
    "- **Pelatihan Asinkron (Asynchronous Training)**: Semua pekerja melatih data input secara mandiri dan memperbarui variabel secara asinkron. Mereka menyinkronkan model terdistribusi melalui arsitektur *parameter server*.\n",
    "\n",
    "### 3. **Strategi Distribusi TensorFlow**\n",
    "TensorFlow mendukung berbagai strategi untuk membantu Anda melatih model menggunakan skenario yang disebutkan sebelumnya. Beberapa strategi utama yang didukung adalah:\n",
    "\n",
    "#### a. **MirroredStrategy**\n",
    "Strategi ini umumnya digunakan untuk pelatihan terdistribusi pada satu komputer dengan beberapa GPU. \n",
    "- **Cara Kerja**: Model dibuat salinannya di setiap GPU dan variabelnya dipantulkan. Setelah setiap epoch pelatihan, parameter yang dipelajari digabungkan menggunakan algoritma *all-reduce* di antara perangkat.\n",
    "\n",
    "#### b. **TPUStrategy**\n",
    "Mirip dengan *MirroredStrategy*, namun menggunakan core TPU (Tensor Processing Unit) alih-alih GPU. Jika Anda memiliki kluster TPU yang tersedia, Anda bisa menggunakan strategi ini.\n",
    "\n",
    "#### c. **MultiWorkerMirroredStrategy**\n",
    "Digunakan ketika Anda memiliki beberapa mesin dalam jaringan. Setiap mesin ini dapat memiliki jumlah GPU yang berbeda. Strategi ini mereplikasi dan memantulkan model ke setiap pekerja, bukan ke setiap perangkat GPU, dan menggunakan algoritma *all-reduce* berdasarkan pengaturan perangkat keras.\n",
    "\n",
    "#### d. **CentralStorageStrategy**\n",
    "Biasanya digunakan pada satu mesin. Namun, ketika ada banyak GPU pada mesin tersebut, variabel tidak dipantulkan di antara GPU, melainkan disimpan dan diproses oleh CPU. Jika hanya ada satu GPU, GPU digunakan karena variabel yang dipantulkan tidak diperlukan.\n",
    "\n",
    "#### e. **ParameterServerStrategy**\n",
    "Strategi ini menggunakan mesin khusus yang berfungsi sebagai penyimpan data independen, seperti *parameter server*. Variabel model (misalnya bobot, bias, atau filter) disimpan di tempat terpusat. Beberapa mesin dalam jaringan akan menjadi pekerja yang melakukan pelatihan, sementara mesin lainnya adalah parameter server.\n",
    "\n",
    "#### f. **Default Strategy**\n",
    "Merupakan strategi default yang tidak memerlukan perangkat keras atau jaringan khusus. Ini hanya mengizinkan Anda untuk membuat kode distribusi tanpa memerlukan banyak GPU atau mesin.\n",
    "\n",
    "#### g. **OneDeviceStrategy**\n",
    "Strategi ini memfokuskan semua data dan parameter pelatihan pada satu perangkat. Ini cocok untuk prototyping kode pada satu mesin yang nantinya bisa diterapkan pada platform terdistribusi.\n",
    "\n",
    "### 4. **Kesimpulan**\n",
    "Setiap strategi memiliki kegunaan yang berbeda, tergantung pada perangkat keras yang tersedia dan kebutuhan pelatihan model Anda. Dalam kursus ini, kita akan memulai dengan menggunakan *MirroredStrategy* untuk pelatihan distribusi, yang akan memberikan gambaran dasar tentang bagaimana strategi-strategi lain bekerja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Strategi Distribusi Training di TensorFlow**\n",
    "\n",
    "Kita telah mempelajari strategi **Merit Strategy** dan **TPU Strategy** dalam kursus ini, terutama karena keduanya memanfaatkan perangkat keras yang mudah diakses dan dapat dilakukan dengan mudah menggunakan Colab. Namun, ada beberapa strategi lain yang juga penting untuk dipahami. Berikut penjelasannya.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. OneDeviceStrategy**\n",
    "Strategi ini digunakan untuk melakukan training pada **satu perangkat tertentu**.  \n",
    "#### **Kelebihan**:\n",
    "- **Fleksibilitas**: Memilih perangkat seperti `GPU:0`.\n",
    "- **Testing**: Berguna untuk menguji kode sebelum didistribusikan ke beberapa perangkat.\n",
    "\n",
    "**Cara Menggunakan**:  \n",
    "- Identifikasi nama perangkat target (contoh: `GPU:0`).\n",
    "- Deklarasikan strateginya.  \n",
    "Kode lainnya tetap bekerja seperti pada **Custom Training**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. MultiWorkerMirroredStrategy**\n",
    "Strategi ini digunakan untuk **training pada banyak GPU di beberapa mesin**.\n",
    "\n",
    "#### **Fitur Penting**:\n",
    "- **Fault Tolerance**: Menggunakan **checkpoints** untuk melanjutkan training jika terjadi gangguan.\n",
    "- **Sinkronisasi Kompleks**: Menggunakan **CollectiveOps** untuk menjaga semua perangkat tetap sinkron.\n",
    "\n",
    "#### **Ilustrasi Kasus**:\n",
    "- **2 Mesin**:  \n",
    "  - Masing-masing memiliki **1 CPU** dan **2 GPU**.\n",
    "- Dengan strategi ini, training dapat didistribusikan secara efektif.\n",
    "\n",
    "###### **Worker Roles**:\n",
    "- **Worker**: Menangani tugas seperti **training** atau **input pipelines**.\n",
    "- **Chief Worker**: Memiliki tanggung jawab lebih, seperti menyimpan **checkpoints**.\n",
    "\n",
    "**Spesifikasi Cluster**:\n",
    "```json\n",
    "{\n",
    "  \"cluster\": {\n",
    "    \"worker\": [\n",
    "      \"host1:port1\",\n",
    "      \"host2:port2\"\n",
    "    ]\n",
    "  },\n",
    "  \"task\": {\n",
    "    \"type\": \"worker\",\n",
    "    \"index\": 0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "3. CentralStorageStrategy\n",
    "Karakteristik:\n",
    "Variable Storage: Disimpan dalam CPU memory.\n",
    "Komputasi: Dilakukan di GPU untuk menghemat memori GPU.\n",
    "4. ParameterServerStrategy\n",
    "Gabungan Keunggulan:\n",
    "Komputasi: Dilakukan di semua GPU.\n",
    "Variable Storage: Menggunakan distributed database yang disebut parameter server.\n",
    "Ringkasan\n",
    "Strategi yang Dibahas:\n",
    "OneDeviceStrategy: Training di satu perangkat.\n",
    "MultiWorkerMirroredStrategy: Training di banyak GPU pada beberapa mesin.\n",
    "CentralStorageStrategy: Variabel di CPU, komputasi di GPU.\n",
    "ParameterServerStrategy: Distribusi variabel melalui server parameter.\n",
    "Penutup:\n",
    "Kursus ini memberikan pemahaman tentang berbagai strategi distribusi di TensorFlow, termasuk:\n",
    "\n",
    "Training dengan GPU dan TPU.\n",
    "Akses hingga 8 core TPU di Colab untuk mempercepat training.\n",
    "Dengan alat ini, Anda dapat membawa model Anda ke level berikutnya!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
