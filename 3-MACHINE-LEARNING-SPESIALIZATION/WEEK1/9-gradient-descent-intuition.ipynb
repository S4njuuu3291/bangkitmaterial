{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementasi Algoritma Gradient Descent\n",
    "\n",
    "Mari kita lihat bagaimana kita dapat mengimplementasikan algoritma **Gradient Descent**. Saya akan menuliskan algoritma gradient descent di bawah ini.\n",
    "\n",
    "### Algoritma Gradient Descent\n",
    "Pada setiap langkah, parameter \\( w \\) diperbarui dengan nilai \\( w \\) yang lama dikurangi dengan \\(\\alpha\\) (learning rate) dikali dengan turunan dari fungsi biaya \\( J(w, b) \\) terhadap \\( w \\). Notasi ini bisa dituliskan sebagai:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "\n",
    "Apa yang dimaksud dengan ekspresi ini adalah, kita mengambil nilai parameter \\( w \\) yang sekarang dan mengubahnya sedikit (seperti yang ditunjukkan di sisi kanan), yaitu mengurangi dengan \\(\\alpha\\) dikali dengan turunan tersebut.\n",
    "\n",
    "Jika Anda merasa ada banyak hal yang perlu dipahami dalam persamaan ini, tenang saja, tidak perlu khawatir. Kita akan membahasnya bersama-sama.\n",
    "\n",
    "### Operator Penugasan (Assignment Operator)\n",
    "Mari kita mulai dengan membahas tanda **sama dengan** (`=`) yang digunakan di sini. Dalam konteks ini, tanda sama dengan berfungsi sebagai **operator penugasan**. Artinya, jika kita menulis `w = w - alpha * dJ_dw`, kita sedang mengatakan bahwa nilai \\( w \\) yang baru adalah hasil dari operasi tersebut. Dalam Python dan bahasa pemrograman lainnya, jika kita menulis `a = c`, itu berarti kita mengambil nilai dari `c` dan menyimpannya di variabel `a`.\n",
    "\n",
    "Sebagai contoh, jika kita menulis `a = a + 1`, ini berarti kita menaikkan nilai `a` sebesar 1. Operator penugasan ini berbeda dari **pernyataan kebenaran** dalam matematika. Dalam matematika, jika kita menulis `a = c`, itu berarti kita sedang mengklaim bahwa `a` dan `c` sama.\n",
    "\n",
    "Namun, dalam bahasa pemrograman, untuk memeriksa apakah dua nilai sama, kita menggunakan `==`, misalnya `a == c` untuk memeriksa kesetaraan antara `a` dan `c`.\n",
    "\n",
    "### Memahami Simbol dalam Persamaan\n",
    "Mari kita lihat simbol dalam persamaan gradient descent ini. Simbol \\( \\alpha \\) di sini adalah huruf Yunani Alpha yang dalam konteks ini disebut **learning rate** (tingkat pembelajaran). **Learning rate** biasanya adalah angka positif kecil antara 0 dan 1, misalnya \\( \\alpha = 0.01 \\).\n",
    "\n",
    "Apa yang dilakukan oleh \\( \\alpha \\) adalah mengontrol seberapa besar langkah yang diambil saat menuruni gradien. Jika \\( \\alpha \\) sangat besar, maka itu berarti algoritma akan mengambil langkah besar, yang bisa berisiko overshoot. Jika \\( \\alpha \\) kecil, maka kita akan mengambil langkah yang lebih kecil dan hati-hati.\n",
    "\n",
    "Kita akan membahas lebih lanjut tentang bagaimana memilih nilai \\( \\alpha \\) yang baik nanti.\n",
    "\n",
    "### Turunan dari Fungsi Biaya\n",
    "Term ini adalah **turunan** dari fungsi biaya \\( J \\). Jangan khawatir tentang rincian turunan ini saat ini. Nanti, kita akan membahas lebih lanjut tentang bagaimana turunan ini bekerja. Tetapi untuk saat ini, Anda bisa memikirkan turunan ini sebagai petunjuk arah untuk langkah kecil yang ingin Anda ambil.\n",
    "\n",
    "Dengan kombinasi **learning rate** \\( \\alpha \\), turunan ini juga menentukan ukuran langkah yang diambil. Turunan ini memberi tahu kita arah mana yang harus diambil saat melakukan pembaruan.\n",
    "\n",
    "### Pengenalan Derivatif (Turunan) dalam Kalkulus\n",
    "Perlu diingat bahwa derivatif berasal dari kalkulus. Meskipun Anda tidak familiar dengan kalkulus, tidak perlu khawatir. Anda tetap bisa memahami apa yang terjadi pada term derivatif ini hanya dengan melihat video ini dan video berikutnya.\n",
    "\n",
    "### Pembaruan Parameter \\( w \\) dan \\( b \\)\n",
    "Model Anda memiliki dua parameter, bukan hanya \\( w \\), tetapi juga \\( b \\) (bias). Pembaruan parameter \\( b \\) dilakukan dengan cara yang sangat mirip dengan \\( w \\). Jadi, \\( b \\) diperbarui dengan cara:\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Setelah kita melakukan pembaruan untuk kedua parameter ini, \\( w \\) dan \\( b \\), kita akan mengulang langkah-langkah ini hingga algoritma **konvergen**. Konvergen di sini berarti kita mencapai titik minimum lokal dimana nilai parameter \\( w \\) dan \\( b \\) tidak berubah signifikan lagi setelah beberapa langkah pembaruan.\n",
    "\n",
    "### Pembaruan Simultan\n",
    "Satu hal yang perlu diperhatikan adalah, untuk **gradient descent**, kita ingin **memperbarui** \\( w \\) dan \\( b \\) secara bersamaan. Ini berarti bahwa pembaruan untuk kedua parameter tersebut terjadi pada waktu yang bersamaan, seperti yang digambarkan dalam persamaan berikut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
