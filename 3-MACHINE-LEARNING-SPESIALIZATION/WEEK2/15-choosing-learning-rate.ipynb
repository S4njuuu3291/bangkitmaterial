{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menentukan Learning Rate yang Tepat\n",
    "\n",
    "Pemilihan **learning rate** yang tepat sangat penting untuk memastikan **algoritma pembelajaran** berjalan efisien. Jika terlalu kecil, proses akan berjalan sangat lambat, sedangkan jika terlalu besar, proses mungkin tidak akan konvergen sama sekali.\n",
    "\n",
    "#### Indikator Kesalahan pada Gradient Descent\n",
    "Jika Anda memplot **fungsi biaya (cost function)** untuk beberapa iterasi dan melihat bahwa biaya **kadang naik, kadang turun**, ini adalah indikasi bahwa **gradient descent** tidak bekerja dengan benar. Hal ini bisa disebabkan oleh dua hal:\n",
    "1. **Bug dalam kode**.\n",
    "2. **Learning rate** yang terlalu besar.\n",
    "\n",
    "#### Ilustrasi Overshoot\n",
    "Untuk lebih memahaminya, bayangkan sumbu vertikal adalah **fungsi biaya J**, dan sumbu horizontal mewakili parameter seperti **w_1**. Jika **learning rate** terlalu besar, setiap langkah pembaruan parameter bisa **melewati minimum** dan malah menjauh dari titik optimal.\n",
    "\n",
    "- **Overshoot**: Jika langkah pembaruan terlalu besar, Anda akan melewati minimum dan berakhir di posisi yang lebih tinggi dari sebelumnya. Hal ini bisa terjadi berulang kali, menyebabkan **fungsi biaya naik**.\n",
    "  \n",
    "##### Solusi\n",
    "Untuk memperbaiki ini, gunakan **learning rate** yang lebih kecil. Dengan langkah yang lebih kecil, proses pembaruan akan secara konsisten menurun, mendekati minimum global.\n",
    "\n",
    "#### Peningkatan Biaya Secara Konsisten\n",
    "Jika fungsi biaya terus meningkat setelah setiap iterasi, ini juga bisa menunjukkan bahwa **learning rate** terlalu besar, atau mungkin ada kesalahan dalam kode.\n",
    "\n",
    "##### Contoh Kesalahan Kode\n",
    "Jika kode pembaruan parameter **w_1** salah, misalnya:\n",
    "\n",
    "```python\n",
    "w_1 = w_1 + Alpha * turunan\n",
    "Langkah ini menambah parameter dengan turunan, yang seharusnya menguranginya. Dengan demikian, fungsi biaya J malah semakin jauh dari minimum global.\n",
    "\n",
    "Debugging Gradient Descent\n",
    "Salah satu tips debugging yang sering digunakan adalah mencoba learning rate yang sangat kecil. Jika dengan Alpha yang kecil pun fungsi biaya masih tidak menurun di setiap iterasi, kemungkinan besar ada bug dalam kode Anda.\n",
    "\n",
    "Trade-off dalam Learning Rate\n",
    "Jika learning rate terlalu kecil, meskipun proses berjalan dengan benar, gradient descent akan membutuhkan banyak iterasi untuk mencapai konvergensi. Oleh karena itu, penting untuk menemukan Alpha yang tepat.\n",
    "\n",
    "Teknik Pemilihan Learning Rate\n",
    "Ketika menjalankan gradient descent, cobalah beberapa nilai Alpha. Sebagai contoh, mulailah dengan 0.001, lalu coba nilai yang 10 kali lebih besar, seperti 0.01 atau 0.1, dan seterusnya.\n",
    "\n",
    "Untuk setiap pilihan Alpha, jalankan gradient descent selama beberapa iterasi dan plot fungsi biaya J sebagai fungsi dari jumlah iterasi.\n",
    "Pilih Alpha yang menurunkan biaya dengan cepat dan konsisten.\n",
    "Pencarian Optimal Alpha\n",
    "Cara terbaik untuk memilih learning rate adalah dengan mencoba berbagai nilai yang berkisar pada 3 kali lebih besar dari nilai sebelumnya:\n",
    "\n",
    "Mulai dari 0.001, kemudian tingkatkan menjadi 0.003, kemudian 0.01, dan seterusnya.\n",
    "Setelah menemukan nilai yang terlalu kecil dan terlalu besar, pilih learning rate terbesar yang masih memberikan hasil konvergen.\n",
    "Kesimpulan\n",
    "Teknik ini dapat membantu Anda menentukan learning rate yang tepat untuk model Anda. Dalam lab opsional berikut, Anda bisa bereksperimen dengan feature scaling dan melihat bagaimana pilihan learning rate memengaruhi pelatihan model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
